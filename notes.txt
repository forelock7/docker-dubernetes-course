# Create image base on 'node' image (latest version), pulling it from hub.docker
FROM node

# Change working dir to '/app'(create '/app' and go inside it), which means that all further commands will be executed from '/app'
WORKDIR /app

# Optimization for executing layers. First install all depandancies then coppy all code source in case code would be changed
COPY package.json /app

# Run command
RUN npm install

# Copy all files/folders from '.' path (root, where Dockerfile exists) to '.' path (root) in container
# COPY . .
# if WORKDIR set up we can use either 'COPY . .' or 'COPY . ./' or 'COPY . /app'
COPY . /app

# Let docker know that when container starts we want to expose port 80 to local machine, which runs this contaier
# It just clarifes port, but actually does nothing
# EXPOSE 80

# Set some default arguments
ARG DEFAULT_PORT=80

# Set environmental variable and default value
ENV PORT $DEFAULT_PORT

# Use environmental variable PORT
EXPOSE $PORT

# Anonymous volume. It will be deleted if remove container. Can be provided during 'docker run -v /app/feedback'  
# VOLUME [ "/app/feedback" ]

# It starts app during image creation/building, but it's inncorrect because image is just template. Do not do it!
# RUN node server.js

# It runs command only during start of container
CMD ["node", "server.js"]

----------------------------------------------------------------------------------------------------------------
- Build docker image base Dickerfile in root
docker build .

- Run docker container from image. You can use only few first ID sympols like "f5dffbb96bb3ad76ef4c7"
docker run f5dffbb96bb3ad76ef4c781f96db877dd57d570ef672a77b6bb7e3651b9a32a2

- Get all running docker containers
docker ps

- Get all existing docker containers
docker ps -a

- Stop container by name
docker stop friendly_elion

- Create NEW container and Expose/Publish(-p) local port 3000 (which port shpould be accessable) with internal port 80 inside container
docker run -p 3000:80 ae4e16965ad800a7faa8f07d66148249fe68daa88344938616207a3b3e7fdbfb

- Docker caches all instruction(lines in Dockerfile). It re-builds only changed layer. Therefore it can re-builds faster second time than very first.
- If one layer changed, Docker re-executes this one and all following.
- Every instruction in an image creates a cacheable layer.

- Check available option for cirtain commands
docker COMMAND --help
docker build --help 

- Start existing container, which was stopped before. Do not need create new container(docker run ...) if you didn't change image.
docker start container_name_id

- Attached mode - default for 'docker run'. Listening output of container, for example consol output.
docker run image_id
docker start -a container_name_id
docker attach container_name_id

- Detached mode - default for 'docker start'. The process in terminal finishes immediately. The container is running in the background.
docker start container_name_id
docker run -d container_name_id

- See the last logs in container
docker logs container_name_id

- See the last logs and follow log output in container
docker logs -f container_name_id

- Allocate a pseudo-TTY and keep STDIN open even if not attached. So we can input something via console if app needs.
docker run -it e38a6e781747027fb1ceefe9fa

- Start container with open STDIN(--attach and --interactive)
docker start -a -i awesome_fermi

- Remove container, but you need to stop it before
docker stop container_name_id
docker rm container_name_1 container_name_2 container_name_3

- Get list of all images
docker images

- Remove images, but you need to remove container before
docker rmi e38a6e781747

- Remove all unused images
docker image prune

- Automatically remove the container (--rm) when it exits (in case below it will be removed after stoping)
- Actually usefull when you need to rebuild image after some changes
docker run -p 3000:80 -d --rm 138689ba4848
docker stop busy_moser

- Display detailed information on one or more images
docker image inspect 138689ba4848

- Copy file from local machine to docker container
docker cp dummy/file.txt container_name:/app/test

- Copy file from docker container to local machine
docker cp container_name:/app/test dummy/file.txt

- Run/Create container with name 'goalsapp'
docker run -p 3000:80 -d --rm --name goalsapp 138689ba48485b88d3b8befe3952a1aa

- Image tag: "<name>:<tag>". For example "node:12"
-t Name and optionally a tag (format: "name:tag")
docker build -t goals:latest .
docker run -p 3001:80 -d --rm --name goalsapp2 goals:latest

- Rename image name:tag 
docker tag nodeapp:latest forelock/node-hello-world:latest

- Push docker image to hub.docker.commands (create before repo on docker hub)
docker login
docker push forelock/node-hello-world:latest

- Pull image from hub.docker.com
docker pull forelock/node-hello-world

////////////////////////////////////////////////////////////////////////////////////////////
Section 3: Managing Data & Working with Volumes
////////////////////////////////////////////////////////////////////////////////////////////

- Get list of volumes
docker volume ls

- Anonymous volumes created in Dockerfile, but named volumes created during run container
docker run -d -p 3000:80 --rm --name feedback-app -v feedback:/app/feedback feedback-node:volumes

- Delete volume
docker volume rm VOL_NAME
docker volume prune

- Type of data storage: Volumes(Anonymous and Named - managed by docker) and Bind Mounts(Managed by you)
- check before resources are shared with docker(local dir should be shared - Docker Desktop/settings/resources
- Anonymous Volume: "-v /app/node_modules" - overides particular dirs (if their more specific) from Bind Mount to save something that was created during image building
+ 1) Created specifically for a single container
+ 2) Survives container shutdown / restart - unless --rm is used
+ 3) Can not be shared across containers
+ 4) Since it's anonymous, it can't be re-used for same container (even same image)
- Named Volume: "-v feedback:/app/feedback" - for saving results when we stop/remove containers.
+ 1) Created in general - not tied to any specific container
+ 2) Survives container shutdown / restart - removal via Docker clarifes
+ 3) Can be shared across containers
+ 4) Can be re-used for same container (across restarts)
- Bind Mount: "-v "/Users/volodymyrchubenko/Documents/Courses/Docker-Kubernetes-Course/5-data-volumes-01-starting-setup:/app""" - for using code outside container, helps us to update code and do not re-build image.
+ 1) Location on host file system, not tied to any sppecific container
+ 2) Survives container shutdown / restart - removal on host fs
+ 3) Can be shared across containers
+ 4) Can be re-used for same container (across restarts)
- In this case DO NOT NEED VOLUME instaraction in Dockerfile
docker run -d --rm -p 3000:80 --name feedback-app -v feedback:/app/feedback -v "/Users/.../5-data-volumes-01-starting-setup:/app" -v /app/node_modules feedback-node:volumes

- Shortcuts:
macOS / Linux: -v $(pwd):/app
Windows: -v "%cd%":/app

- If we use some specific project like web-server on node.js, we might need to stop and start container again to apply some changes: use dependancy kind of "nodemon"

- Bind Mount Volume is in read/write mode. It can be made as read only by adding ':ro' to the end. So we exclude '/app/temp' and '/app/node_modules' from Bind Mount
docker run -d --rm -p 3000:80 --name feedback-app -v feedback:/app/feedback -v "/Users/volodymyrchubenko/Documents/Courses/Docker-Kubernetes-Course/5-data-volumes-01-starting-setup:/app:ro" -v /app/temp -v /app/node_modules feedback-node:volumes

- Create Named Volume
docker volume create feedback-files

- Get infor about volume:
docker volume inspect feedback-files

- Remove volume if unused (need to stop/remove container before)
docker volume rm feedback-files

- Remove all unused volumes
docker volume prune

- Mostly Bind Mount is not used in production. It just needed for development

- To ignore some files/dirs during 'COPY' instruction need to add them to '.dockerignore' file

- propagate env vars we can use '--env PORT=8000' or '-e PORT=8000 -e HOST=10.32.43.21' in case "ENV PORT 80; EXPOSE $PORT;" in Dockerfile
docker run -d --rm -p 3000:8000 --env PORT=8000 --name feedback-app -v feedback:/app/feedback -v "/Users/volodymyrchubenko/Documents/Courses/Docker-Kubernetes-Course/5-data-volumes-01-starting-setup:/app:ro" -v /app/temp -v /app/node_modules feedback-node:env

- '.env' file can be also created with env vars, but then you need to use '--env-file ./.env' if it's in root
docker run -d --rm -p 3000:8001 --env-file ./.env .....

- Build Arguments is used to set different values during build image (ARG DEFAULT_PORT=80; ENV PORT $DEFAULT_PORT; EXPOSE $PORT;)
docker build -t feedback-node:web-app .    (applied 80 port)
docker build -t feedback-node:dev-app --build-arg DEFAULT_PORT=8000 .  (applied 8000 port)

////////////////////////////////////////////////////////////////////////////////////////////
Section 4: Networking: (Cross-)Container Communication
////////////////////////////////////////////////////////////////////////////////////////////

- Coonection to WEB: Docker container can communicate with WWW without additional configuration

- Connection to Host: If you have locally DB, out of Docker, You should use in Docker instead of 'localhost': 'host.docker.internal'
- For instance: 'mongodb://host.docker.internal:27017/swfavorites'

- Connection to other contaier: just use ip and port of containers

- Get info about container:
docker container inspect mongodb

- Create network:
docker network create favorites-net

- Get all existing docker networks:
docker network ls

- Add containers to the same networ, you can replace ip by container name in code: "mongodb://mongodb:27017/swfavorites"
docker run -d --name mongodb --network favorites-net mongo
docker run --name favorites --network favorites-net -d --rm -p 3000:3000 favorites-node

- Docker Networks actually support different kinds of "Drivers" which influence the behavior of the Network.
- The default driver is the "bridge" driver - it provides the behavior shown in this module (i.e. Containers can find each other by name if they are in the same Network).
- The driver can be set when a Network is created, simply by adding the --driver option.
docker network create --driver bridge my-net
- Of course, if you want to use the "bridge" driver, you can simply omit the entire option since "bridge" is the default anyways.
- Docker also supports these alternative drivers - though you will use the "bridge" driver in most cases:
+ host: For standalone containers, isolation between container and host system is removed (i.e. they share localhost as a network)
+ overlay: Multiple Docker daemons (i.e. Docker running on different machines) are able to connect with each other. Only works in "Swarm" mode which is a dated / almost deprecated way of connecting multiple containers
+ macvlan: You can set a custom MAC address to a container - this address can then be used for communication with that container
+ none: All networking is disabled.
- Third-party plugins: You can install third-party plugins which then may add all kinds of behaviors and functionalities
- As mentioned, the "bridge" driver makes most sense in the vast majority of scenarios.

////////////////////////////////////////////////////////////////////////////////////////////
Section 5: Building Multi-Container Applications with Docker
////////////////////////////////////////////////////////////////////////////////////////////

- Multi-container 
docker run --name mongodb --rm -d -p 27017:27017 mongo
docker build -t goals-node .
docker run --name goals-backend --rm -d -p 80:80 goals-node
docker build -t goals-react .
docker run --name goals-frontend --rm -p 3000:3000 -it goals-react
docker network create goals-net
docker run --name mongodb --rm -d --network goals-net mongo
// change in backend code localhosts to docker container name and re-build images
// still need to publish port 80 because frontend will reach from not container, but from outside - browser
docker run --name goals-backend --rm -d -p 80:80 --network goals-net goals-node
docker run --name goals-frontend --rm -p 3000:3000 -it goals-react 

- Persist data after removing of mongodb
docker run --name mongodb -v data:/data/db -d --rm --network goals-net mongo

- Add authontification to DB, but before update code in backend "mongodb://max:secret@mongodb:27017/course-goals?authSource=admin"
- Remove volume for DB before
docker volume rm data
docker run --name mongodb -v data:/data/db -d --rm --network goals-net -e MONGO_INITDB_ROOT_USERNAME=max -e MONGO_INITDB_ROOT_PASSWORD=secret mongo
docker run --name goals-backend --rm -p 80:80 --network goals-net goals-node

- Run backend app with saving logs dir and possibility to change code on flight
docker run -v /Users/volodymyrchubenko/Documents/Courses/Docker-Kubernetes-Course/7-multi-01-starting-setup/backend:/app -v logs:/app/logs -v /app/node_modules --name goals-backend --rm -d -p 80:80 --network goals-net goals-node
- Add to dev dep 'nodemon', update start script and Dockerfile, re-build image and re-run container

- Set username/password as env variable
+ replace it in code by `mongodb://${process.env.MONGODB_USERNAME}:${process.env.MONGODB_PASSWORD}@mongodb:27017/course-goals?authSource=admin`
+ add env vars in Dockerfile: ENV MONGODB_USERNAME=root \ ENV MONGODB_PASSWORD=secret
+ rebuild image and re-run container
docker run -v /Users/volodymyrchubenko/Documents/Courses/Docker-Kubernetes-Course/7-multi-01-starting-setup/backend:/app -v logs:/app/logs -v /app/node_modules --name goals-backend --rm -d -e MONGODB_USERNAME=max -p 80:80 --network goals-net goals-node

- Run frontend app with possibility to change code on flight
docker run -v /Users/volodymyrchubenko/Documents/Courses/Docker-Kubernetes-Course/7-multi-01-starting-setup/frontend/src:/app/src --name goals-frontend --rm -p 3000:3000 -it goals-react

- To improve image building
+ Add '.dockerignore' file and add at least: node_modules, .git, Dockerfile

////////////////////////////////////////////////////////////////////////////////////////////
Section 6: Docker Compose: Elegant Multi-Container Orchestration
////////////////////////////////////////////////////////////////////////////////////////////

- One configuration File + Orchestration Commands (build, start? stop)
- Docker Compose works with Dockerfile
- Services (Containers):
+ Published Ports
+ Volumes
+ Env vars
+ Networks

- docker-compose version - https://docs.docker.com/compose/compose-file/compose-file-v3/

- Do not need specify network because docker compose provide that default network out of the box

- On macOS and Windows, you should already have Docker Compose installed - it's set up together with Docker there.
- On Linux machines, you need to install it separately.
- These steps should get you there:
1. sudo curl -L "https://github.com/docker/compose/releases/download/1.27.4/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
2. sudo chmod +x /usr/local/bin/docker-compose
3. sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose
4. to verify: docker-compose --version
- Also see: https://docs.docker.com/compose/install/

- Up docker-compose in attached mode: build images, container, volumes, default network
docker-compose up

- Up docker-compose in detached mode
docker-compose up -d

- Down docker-compose: remove container and network
docker-compose down

- Down docker-compose: remove not only container and network, but volumes
docker-compose down -v

- Up docker-compose and force re-building of images (commonly docker-compose doesn't build images if their built before)
docker-compose up --build

- Build images by docker-compose
docker-compose build

////////////////////////////////////////////////////////////////////////////////////////////
Section 7: Working with "Utility Containers" & Executing Commands In Containers
////////////////////////////////////////////////////////////////////////////////////////////

- Execute some commands inside container BESIDES default command of container
docker run -it -d node
docker exec -it docker_container npm init

- Execute some commands inside container OVERRIDE default command of container
docker run -it node npm init

- Create Utility image
FROM node:14-alpine
WORKDIR /app
docker build -t node-util .

- Run node-util container with Bind Mount. It help us to use containers and work with a project as standalone, without any aditional installation on local machine
- If we have 'CMD ["npm", "install"]', this command will be overrided by command in console command
docker run -it -v /Users/volodymyrchubenko/Documents/Courses/Docker-Kubernetes-Course/8-utility-containers:/app node-util npm init

- If we have 'ENTRYPOINT ["npm"]', console command will be appended to "npm". Container is always shut down after execution, but we can run other command because Bind Mount
docker build -t mynpm .
docker run -rm -it -v /Users/volodymyrchubenko/Documents/Courses/Docker-Kubernetes-Course/8-utility-containers:/app mynpm init
docker run -rm -it -v /Users/volodymyrchubenko/Documents/Courses/Docker-Kubernetes-Course/8-utility-containers:/app mynpm install
docker run -rm -it -v /Users/volodymyrchubenko/Documents/Courses/Docker-Kubernetes-Course/8-utility-containers:/app mynpm install express --save

- The same we can do with docker-compose (Run only one particular container from docker-compose)
version: "3.8"
services:
  npm_service:
    build: ./
    stdin_open: true
    tty: true
    volumes:
      - ./:/app
docker-compose run --rm npm_service init

////////////////////////////////////////////////////////////////////////////////////////////
Section 8: A More Complex Setup: A Laravel & PHP Dockerized Project
////////////////////////////////////////////////////////////////////////////////////////////

- If there is no execution command in dockerfile (CMD, ENTRYPOINT), so execution command will be taken from base image

- Specific dockerfile (with different name) placed not in the root of project:
services:
  php:
    build:
      context: ./dockerfiles
      dockerfile: php.dockerfile

- To improve performance during using of Bind Mount: ':delegated' option
    volumes:
      -./src:/var/www/htm:delegated

- Run comoser of php project
docker-compose run --rm composer create-project --prefer-dist laravel/laravel:^8.0 .

- Run particular services(containers) from docker-compose file:
docker-compose up -d server php mysql

- Dependent services can be added to run only by one service, but depended services weren't build
  server:
    image: "nginx:stable-alpine"
    .....
    depends_on:
      - php
      - mysql
docker-compose up -d --build server

- Run dependent services by one service with re-/building their images
docker-compose up -d --build server

- ENTRYPOINT instruction in Dockerfile can be override in docker-compose
  artisan:
    build:
      context: .
      dockerfile: dockerfiles/php.dockerfile
    volumes:
      - ./src:/var/www/html
    entrypoint: ['php', '/var/www/html/artisan']
docker-compose run --rm artisan migrate

- Best practice to keep docker-compose and dockerfiles in project, not use docker-compose file without dockerfile

- Context means not only where Dockerfile is, but also where should docker image build
    build:
      context: .
      dockerfile: dockerfiles/nginx.dockerfile
    ......
    build:
      context: ./dockerfiles
      dockerfile: composer.dockerfile

- AWS first login via SSH, but coppy '.pem' file to the root of project (see the instruction in AWS EC2 how to connect to VM)
chmod 400 example-1.pem
ssh -i "example-1.pem" ec2-user@ec2-16-171-65-79.eu-north-1.compute.amazonaws.com

- Install docker to AWS EC2 machine
sudo yum update -y
sudo yum -y install docker
sudo service docker start
sudo usermod -a -G docker ec2-user
*log out and log in once more
sudo systemctl enable docker
docker version

- To have access to AWS EC2 need to add security group for http - port 

- To update image remotely, need to stop container, pull new latest version of image and re-run container
docker stop 207978fa836c
docker pull forelock/node-example-1
docker run -d --rm -p 80:80 forelock/node-example-1

- Multistage build(dockerfile). For production we need not start frontend application, but build it(see scripts in package.json). Dockerfile.prod is created for this.
FROM node:14-alpine as build
WORKDIR /app
COPY package.json .
RUN npm install
COPY . .
RUN npm run build
// Create new stage
FROM nginx:stable-alpine
// Copy files from 'build' stage
COPY --from=build /app/build /usr/share/nginx/html
EXPOSE 80
CMD ["nginx", "-g", "daemon off;"]

- Build specific dockerfile, not default Dockerfile in the root. './frontend' - context for the build command
docker build -f frontend/Dockerfile.prod ./frontend

- Build specific stage (see previouse Dockerfile example)
docker build --target build ....

////////////////////////////////////////////////////////////////////////////////////////////
Section 11: Getting Started with Kubernetes
////////////////////////////////////////////////////////////////////////////////////////////

- Kubernetes for orchestrating containers deployment
+ Check container's health
+ Scaling number of containers
+ Balance load between the same containers

- Kubernetes is like Docker-Compose for multiple machines

- Pod holds container and volume. Can contain also mutiple containers
- Worker Node contains Pod(Container) and Proxy/Config. Nodes are your machine/virtual instances
- Multiple Pods can be created and removed to scale app
- Master Node is The Control Plane (control other Worker Nodes)
- Master and Worker Nodes create a cluster

- Worker Node, Pods are created by Kubernetes

- Worker Node contains:
+ Pod/s (Container/s + Volume/s)
+ kubelet - communication between Master and Worker Node
+ Docker
+ kube-proxy - managed Node and Pod network communication


- Master Node
+ API server - API for the kubelets to communicate
+ Scheduler - watches for new Pods, selects Worker Nodes to run them on
+ Kube-Controller-Manager - watches and controls Worker Nodes, correct number of Pods and more
+ Cloud-Controller-Manager - like Kube-Controller-Manager BUT for a specific Cloud Provider: knows how to interact with Cloud Provider resources

- Cluster - set of Node machines which are running the Containerized Application (Worker Nodes) or cotrol other Nodes (Master Node)
- Nodes (Master/Worker) - Physical or virtual machine with a certain hardware capacity which hosts one or mutiple Pods and communicates with the Cluster
- Pods - hold the actual running App Container + their required resources (e.g. volumes)
- Containers - Normal Docker Containers
- Services - a logical set (group) of Pods with a unique, Pod- and Container- independent IP address

////////////////////////////////////////////////////////////////////////////////////////////
Section 12: Kubernetes in Action - Diving into the Core Concepts
////////////////////////////////////////////////////////////////////////////////////////////

- Kubermatic - tool that can help to creat all required infrustructure of Kubernetes
- Kubernetes does not create infrustructure. It just manages it.
- kubectl - tool - sends instruction to the cluster (a new deployment). Master Node applies those command to Worker Nodes
- Minikube - tool helps run kubernetes locally

- kubectl installation with Homebrew (https://kubernetes.io/docs/tasks/tools/install-kubectl-macos/)
brew install kubectl
- Test installation
kubectl version --client

- Install Hypervisor, for instance: VirtualBox(https://www.virtualbox.org/)

- Install Minicube (https://minikube.sigs.k8s.io/docs/start/)
brew install minikube
- Start minikube 
minikube start (by default in docker driver - prefered!!!!0
minikube start --driver=docker
minikube start --driver=virtualbox (unsupported on macOS >13)
- Check status after installation
minikube status
- Minikube dashboard
minikube dashboard
- Delete all minikube clusters
minikube delete --all

- Kubernetes works with Objects:
+ Pods
+ Deployments
+ Services
+ Volume, etc

- Kubernetes objects can be created Imperatively or Declaratively
 
- 'Pod' - smallest utit of Kubernetes
+ contains and run one or multiple containers (commonly 1 pod = 1 container)
+ contains shared resources (e.g. volumes)
+ has a cluster-internal IP by default (containers in pod can use 'localhost')

- 'Deployment' - controls (multiple) Pods. It is used to set up the desired end state
+ set a desired state, Kebernetes then changes the actual state. Define which Pods and containers to run and the number of instances
+ can be paused, deleted and rolled backend
+ can be scaled dynamically and automatically. You can change the number of desired Pods as needed

- Deploy first simple app in kubernetes(12 module)
cd 12-<module></module>...
docker build -t kub-first-app .
// create first remote repo on docker registry - forelock/kub-first-app, because minikube works ONLY WITH REMOTE repos!!!
docker tag kub-first-app forelock/kub-first-app
docker push forelock/kub-first-app
kubectl create deployment first-app --image=forelock/kub-first-app
kubectl get deployments
kubectl get pods

- 'Service' - exposes Pods to the Cluster or Externally. Pods have an internal IP by default - it changes when a Pod is replaced, so:
+ group Pods with a shared IP
+ can allow external access to Pods (default internal only)

- Create Service and expose port from container
kubectl expose deployment first-app --type=LoadBalancer --port=8080
kubectl get services

- type of exposing:
+ --type=ClusterIP (by default and created automatically)
+ --type=NodePort
+ --type=LoadBalancer 

- Open deployed app locally run following command and get :
minikube service first-app
|-----------|-----------|-------------|---------------------------|
| NAMESPACE |   NAME    | TARGET PORT |            URL            |
|-----------|-----------|-------------|---------------------------|
| default   | first-app |        8080 | http://192.168.49.2:31877 |
|-----------|-----------|-------------|---------------------------|
🏃  Starting tunnel for service first-app.
|-----------|-----------|-------------|------------------------|
| NAMESPACE |   NAME    | TARGET PORT |          URL           |
|-----------|-----------|-------------|------------------------|
| default   | first-app |             | http://127.0.0.1:54774 |
|-----------|-----------|-------------|------------------------|
🎉  Opening service default/first-app in default browser...
❗  Because you are using a Docker driver on darwin, the terminal needs to be open to run it.

- Scaling pods in specific deployment (create 3 the same pods). It helps if one pod crash, other will work
kubectl scale deployment/first-app --replicas=3

- Update deployment with new changes(update image and push it into remote repo). Pay attention version of image must be changed(tag ':2'), otherwise deployment will not pull new image version!!!!
docker build -t forelock/kub-first-app:2 .
docker push forelock/kub-first-app:2
kubectl set image deployment/first-app kub-first-app=forelock/kub-first-app:2
// view/check updating status
kubectl rollout status deployment/first-app
- Note if you use config file just choose one from next option for not specifying version
1
image: forelock/kub-first-app:latest
2
image: forelock/kub-first-app
          imagePullPolicy: Always

- Roll back bad changes after setting them into deployment:
kubectl rollout undo deployment/first-app

- Get history of deployments:
kubectl rollout history deployment/first-app

- Get detailed info about particular revision (to see for example image version)
kubectl rollout history deployment/first-app --revision=3

- Roll bach to specific revision
kubectl rollout undo deployment/first-app --to-revision=1

- Delete deployment
kubectl delete deployment first-app

- Declarative Kubernetes Usage uses configuration file
- Look at internet which latest apiVersion should be used for deployment yaml (apiVersion: apps/v1)

- Create deployment based on configuration file
kubectl apply -f=deployment.yaml
kubectl apply -f deployment.yaml

- Create Service by config file:
kubectl apply -f service.yaml
kubectl get services
minikube service backend

- To update deployment just make changeand and re-apply

- Delete deployments also can be deleted by config 
kubectl delete -f=deployment.yaml -f=service.yaml

- Multiple Config file: just put all confog (service and deployment) into one single file, separated by '---'. Service first.
kubectl apply -f=master-deployment.yaml
minikube service backend

- delete objects by labels. Following code will delete service with file 12-kub-action-01-starting-setup/service.yaml
kubectl delete deployments,services -l group=example

- you can configure the way how kubernetes will check health of pods, instead of default mechanism
      containers:
        - name: second-node
          image: forelock/kub-first-app:2
          livenessProbe:
            httpGet:
              path: /
              port: 8080
            periodSeconds: 10
            initialDelaySeconds: 5

////////////////////////////////////////////////////////////////////////////////////////////
Section 13: Managing Data & Volumes with Kubernetes
////////////////////////////////////////////////////////////////////////////////////////////

- Local volumes and cloud-provider volumes
- Volumes are the part of pod

- Define volume and mount it in specific contaimer in yaml file. 'emptyDir' - one of the type of volumes in Kubernetes
    spec:
      containers:
        - name: story
          image: forelock/kub-data-demo:1
          volumeMounts:
            - mountPath: /app/story
              name: story-volume
      volumes:
        - name: story-volume
          emptyDir: {}

- 'emptyDir' driver creates empty directory per pod. So if we use replicas, will get several directories in different pods

- 'hostPath' type directory on local machine. 'Directory' - uses existing dir or 'DirectoryOrCreate' - uses existing dir or create one if it's absent
      volumes:
        - name: story-volume
          hostPath:
            path: /data
            type: DirectoryOrCreate

            